<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ECLAIR Robotics ‚Äì PCR Automation Vision System | Aryaan Saxena</title>
  <link rel="stylesheet" href="style.css" />

  <!-- Project-page layout styles (same pattern as IoT/TREL) -->
  <style>
    body {
      background: #ffffff;
      color: #111111;
    }

    .project-hero {
      max-width: 900px;
      margin: 80px auto 40px;
      padding: 0 20px;
      text-align: left;
    }

    .project-hero h1 {
      font-size: 2.4rem;
      margin-bottom: 0.4rem;
      color: #2563eb;
    }

    .project-subtitle {
      color: #555555;
      max-width: 650px;
    }

    .section {
      max-width: 900px;
      margin: 0 auto 40px;
      padding: 0 20px;
    }

    .section-title {
      font-size: 1.6rem;
      margin-bottom: 0.75rem;
      color: #2563eb;
    }

    .project-text {
      line-height: 1.6;
      color: #111111;
    }

    .project-list {
      margin-top: 0.5rem;
      padding-left: 1.3rem;
    }

    .project-list li {
      margin-bottom: 0.25rem;
      color: #111111;
    }

    .main-image {
      width: 100%;
      border-radius: 16px;
      display: block;
      margin: 0 auto;
      object-fit: cover;
    }

    .video-wrapper {
      margin-top: 12px;
    }

    .project-video {
      width: 100%;
      max-width: 900px;
      border-radius: 16px;
      display: block;
      margin: 0 auto;
    }

    .video-caption {
      margin-top: 6px;
      font-size: 0.95rem;
      color: #4b5563;
    }

    .github-button {
      display: inline-block;
      margin-top: 8px;
      padding: 10px 18px;
      border-radius: 999px;
      text-decoration: none;
      background: #3b82f6;
      color: #f9fafb;
      font-weight: 500;
    }

    .github-button:hover {
      background: #2563eb;
    }

    footer {
      text-align: center;
      padding: 40px 0 20px;
      font-size: 0.9rem;
      color: #6b7280;
    }

    a {
      color: #1d4ed8;
    }
  </style>
</head>

<body>
  <!-- Top gradient from main site -->
  <div class="top-gradient"></div>

  <!-- NAVBAR -->
  <header class="nav-header">
    <div class="nav-left"><a href="index.html" class="back-home">Aryaan Saxena</a></div>
    <nav class="nav-right">
      <a href="index.html#featured">Featured</a>
      <a href="index.html#projects">Projects</a>
      <a href="index.html#skills">Skills</a>
      <a href="index.html#about">About</a>
      <a href="index.html#contact">Contact</a>
    </nav>
  </header>

  <!-- HERO -->
  <section class="project-hero">
    <h1>ECLAIR Robotics ‚Äì PCR Automation Vision System</h1>
    <p class="project-subtitle">
      Teaching a PCR robot to read tiny pipette markings so doctors and scientists don‚Äôt have to.
    </p>
  </section>

  <!-- MAIN IMAGE -->
  <section class="section">
    <!-- üîµ Replace this src with your main ECLAIR photo -->
    <img
      class="main-image"
      src="Screenshot 2025-12-08 154545.jpg"
      alt="ECLAIR PCR automation robot operating a micropipette"
    />
  </section>

  <!-- OVERVIEW -->
  <section class="section">
    <h2 class="section-title">Overview</h2>
    <p class="project-text">
      At <strong>ECLAIR Robotics</strong> I worked on a PCR automation robot that operates micropipettes and reads
      the tiny volume markings on the side to assist doctors and scientists. My main job was to make the robot
      <strong>‚Äúsee‚Äù those numbers reliably enough</strong> that we could trust it instead of a human double-checking
      every pipette.
      <br><br>
      I built a number-recognition system in <strong>Python</strong> using <strong>OpenCV</strong> and
      <strong>Tesseract OCR</strong>. The tricky part was getting consistent crops of the pipette markings, so I
      implemented a custom <strong>parallel prefix-minimum algorithm</strong> (in Python) to deskew, align, and
      isolate the printed scale before running OCR. From there I engineered a full preprocessing pipeline‚Äîgrayscale,
      denoising, thresholding, dilation/erosion‚Äîto clean the image before text extraction.
      <br><br>
      After a lot of iteration on thresholds and heuristics, I pushed the system to around
      <strong>95% detection accuracy</strong> on our internal test set. Along the way I also contributed simple
      Arduino‚ÄìPython serial commands for basic robot motion and used <strong>GitHub</strong> plus
      <strong>unit tests</strong> to keep the codebase from collapsing as we experimented on new approaches.
    </p>
  </section>

  <!-- HOW IT WORKS -->
  <section class="section">
    <h2 class="section-title">How It Works</h2>
    <p class="project-text">
      The vision pipeline that feeds the robot is roughly:
    </p>
    <ul class="project-list project-text">
      <li>Capture a close-up frame of the micropipette and its volume markings.</li>
      <li>Run a <strong>prefix-minimum‚Äìbased alignment step</strong> to deskew the pipette body and crop around the printed scale.</li>
      <li>Apply a preprocessing stack in OpenCV: <strong>grayscale ‚Üí blur/denoise ‚Üí adaptive threshold ‚Üí morphology</strong>.</li>
      <li>Feed the cleaned region into <strong>Tesseract OCR</strong> to read the current volume marking.</li>
      <li>Validate readings with simple sanity checks and send results over <strong>serial</strong> back to the robot controller.</li>
    </ul>
    <p class="project-text">
      In practice, this meant constantly balancing ‚Äúfancy vision tricks‚Äù with what actually worked on noisy,
      real-world lab photos: tuning kernel sizes, trying different thresholding strategies, and writing quick
      visualization scripts to see where the pipeline was failing.
    </p>
  </section>

  <!-- TECHNICAL HIGHLIGHTS -->
  <section class="section">
    <h2 class="section-title">Technical Highlights</h2>
    <ul class="project-list project-text">
      <li>Python vision stack using <strong>OpenCV</strong> and <strong>Tesseract OCR</strong>.</li>
      <li>Custom <strong>parallel prefix-minimum</strong> implementation (Python + C) for crop and deskew.</li>
      <li>Image preprocessing: grayscale, denoising, thresholding, dilation/erosion, and contour analysis.</li>
      <li>~<strong>95% detection accuracy</strong> on internal test images of pipette markings.</li>
      <li>Arduino‚ÄìPython <strong>serial control</strong> for simple robot commands and test automation.</li>
      <li>Collaborative development using <strong>GitHub</strong>, code reviews, and unit tests on key processing steps.</li>
    </ul>
  </section>

  <!-- VIDEO DEMO -->
  <section class="section">
    <h2 class="section-title">Demo</h2>
    <p class="project-text">
      Here‚Äôs a short demo of the system running on real pipette footage and sending readings back to the robot:
    </p>
    <div class="video-container">
      <!-- üîµ Replace VIDEO_ID with your actual YouTube (or other) video ID -->
      <video class="project-video" controls>
        <source src="PCR Research Project Recap.mp4" type="video/mp4" />
        Your browser does not support the video tag.
      </video>
    </div>
  </section>

  <!-- GALLERY -->
  <section class="section">
    <h2 class="section-title">Gallery</h2>
    <div class="gallery-row">
      <div class="gallery-item">
        <!-- üîµ Replace this src with your photo -->
        <img
          class="gallery-img"
          src="Screenshot 2025-12-08 154833.jpg"
          alt="Vision overlay showing pipette markings detected by the system"
        />

        <div class="gallery-item">
        <!-- üîµ Replace this src with your photo -->
        <img
          class="gallery-img"
          src="Screenshot 2025-12-08 154850.jpg"
          alt="Circuit Diagram of System"
        />
      </div>
    </div>
  </section>

  <!-- GITHUB + LINKS (optional) -->
  <section class="section">
    <p style="margin-top: 20px;">
      <a href="index.html" class="back-link">‚Üê Back to Portfolio</a>
    </p>
  </section>

  <footer>
    ¬© 2025 Aryaan Saxena
  </footer>
</body>
</html>
